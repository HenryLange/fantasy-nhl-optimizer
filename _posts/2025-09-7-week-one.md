---
title: "Week 1: Setting Up the Fantasy NHL Optimizer Project"
date: 2025-09-07
---

For the first week of this project, my main goal was to **set up the project and clean the initial data**. This week was all about preparing a solid foundation so that all of the work I want to do in the following weeks with visuals and optimization will go smoothly.

### Step 1: Collecting the Data
I began by searching for what data I was gonna use. I decided that, for now, the best option would be **regular-season data** from the most recent season (2024-2025). I think this is a good starting point, as I will have over a thousand players and a hundred goalies to analyze recent data from. I searched a few different sites like Quanthockey and Moneypuck, but landed on Hockey Reference for downloading player and goalie data. I got the data in CSV format by pasting it into a TextEdit file on my Mac and saving it as a CSV, doing this once for players and once for goalies. For now, I focused on regular-season stats, but playoff data could possibly be used for future analysis.

### Step 2: Loading the Data into R
I imported the CSV files into RStudio and looked at both to get a rough idea of what data I had. The CSV files were far from perfect, so I had to do some work on the data. 

### Step 3: Cleaning the Skater Data
I applied many cleaning steps to make the skater dataset usable for visualization and analysis. These steps include:

1. **Remove unnecessary columns**: Dropped the `Awards` column since it was creating unnecessary `NA`s.  
2. **Remove extra rows**: Deleted the last row, which contained a summary/league average, and not a player.  
3. **Filter multi-team totals**: One interesting problem I ran into was that many players were traded during the season. this resulted in them having a row for their aggregated season stats, but also one line for each team they played for. I ensured each player had a single row representing their total/aggregate stats by filtering out the duplicate rows with half or a third of some players’ stats. Shown below is how I did this step. 

```r
#getting rid of duplicate players
player_stats_2425 <- player_stats_2425 %>%
  group_by(player) %>%
  filter(n_distinct(team) == 1 | grepl("TM", team)) %>%
  ungroup()
```

The filter for “TM” looks for any player that has their team listed as “2TM” or “3TM” since this is where Hockey Reference puts that player’s aggregate stats for the year. By filtering for this, their stats from only one team are removed.

4. **Fix missing numeric values**:
   - There were many `NA`s in my player stats, primarily in the faceoff percentage and shooting percentage columns
   - I replaced `NA`s in **FaceoffPercentage** with `0` for all players (including centers), so calculations later won’t break. I was shocked to see some centers had a 0 faceoff percentage, but most of them just played a very short amount of time and got no faceoffs all season.
   - I replaced `NA`s in **ShootingPercentage** with `0`. These players either had no shots or just had some pretty bad aim.
   - I even had to look up the age of two players that had `NA` values in their age column. I guess 25-year-old Sam Morton and 22-year-old Jack Williams are the chosen ones. 
5. **Rename and standardize columns**: The column names of my initial CSV were not great. Some contained the right words or abbreviation, but then had “…24” or some other number. Some just had the “…24” and no words. I made column names consistent and clear by renaming(e.g., `GP`, `G`, `A`, `PTS`).  

### Step 4: Cleaning the Goalie Data
For goalies, I applied similar steps. I will not re-explain them all, but the steps included:
- Removed unnecessary columns and extra rows
- Checked for missing values and ensured `NA`s were replaced with proper values
- Verified that each row represents a single goalie by filtering.
- renamed the columns

### Step 5: Preparing for Analysis
At this point, the data is ready for analysis and the visuals I plan to make later this week. Instead of saving cleaned CSVs, I kept **all the cleaning steps in my R script**, so I can regenerate the cleaned datasets anytime from the original CSVs. This approach keeps the repository clean and ensures reproducibility.

### My code
My code for this week is posted in the [code](https://github.com/henrylange/fantasy-nhl-optimizer/tree/main/code) section, seen in the navigation bar of this website. I commented throughout the all code to ensure every step is understandable and can be followed.
---

### What’s next?
This week was mainly **data preparation**, but it was crucial (and a bit tedious) to ensure all future steps are built on a reliable foundation. Later in the week, I will test my datasets by trying to make visuals. If they turn out clean, I can trust the data cleaning went well. After that, I plan to start attacking the bigger part of this project. The optimization. 
